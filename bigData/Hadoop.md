# Hadoop

[toc]

## 初学者指南

地址是 [hadoop 初学者指南](https://bigdata2.apachecn.org/#/docs/hadoop-begin-guide/README)

### 0. 前言

以前大型数据集应用复杂分析的能力曾经是大公司和政府机构的专利，但是现在可以通过免费的开源软件（OSS）实现。但由于这一领域似乎很复杂，变化速度也很快，掌握基础知识可能会有点令人望而生畏。这就是这本书的用武之地，它会告诉你 Hadoop 是什么，它是如何工作的，以及你如何使用它从数据中提取价值。

除了对核心 Hadoop 的解释之外，我们还花了几章来探索使用 Hadoop 或与之集成的其它技术。我们的目标不仅是让你了解 Hadoop 是什么，还希望你了解更广泛的技术基础设施的一部分来使用。一种补充技术是使用云计算，特别是亚马逊 Web 服务。在整本书中，将会展示如何使用这些服务来托管你的 Hadoop 工作负载，从而说明你不仅可以处理大量数据，而且实际上不需要购买任何物理硬件就可以实现这一点。

### 1. 说明这一切是怎么回事

Hadoop 不是在真空中创建的；相反，它的存在是因为创建和使用的数据量呈爆炸式增长，而且这种数据洪流不仅出现在大型跨国公司身上，还出现在小型初创公司身上。 与此同时，其他趋势也改变了软件和系统的部署方式，与更传统的基础设施一起使用云资源，甚至优先使用云资源。

#### 大数据处理

环顾一下我们今天拥有的技术，很容易得出这样一个结论：一切都是数据的。不仅生成的数据量在增加，而且增长速度也在加快。从电子邮件到 Facebook 帖子，从购买历史到网络连接，到处都有不断增长的大型数据集。挑战在于如何从这些数据中提取最有价值的方面，有时这意味着寻找数据集里面特定的数据元素，而在其它时候，重点是识别数据片段之间的趋势和关系。

在幕后发生了一种微妙的变化，这一切都是关于以越来越有意义的方式使用数据。

#### 数据的价值

如果大规模的数据处理不能带来可观的回报，为什么要对它进行处理呢，大数据的价值可能会在以下几个方面体现：

- 有些问题只有在足够大的数据集才会给出价值。在没有任何其它因素的情况下，根据一个人的喜好推荐一部电影不太可能非常准确，将人数增加到100人，机会略有增加，如果是1000万人其它人的观看历史记录，发现有用的建议的机会会大大提高
- 与以前的解决方案相比，现在的大数据工具能以更低的成本执行以前昂贵得让人望而却步的数据处理任务
- 大规模数据处理的成本不仅仅是财务费用，延迟也是一个关键因素。一个系统可能能够处理非常多的数据，但是平均处理时间是以周为单位来衡量，那么它可能就没有什么用处了。大数据工具允许在控制处理时间的同时增加数据量，通常是通过将增加的数据量与额外的硬件相匹配来实现（横向扩展）
- 以前关于数据库是什么样子或则它的数据应该怎么设计可能需要重新考虑了，以满足最大的数据问题的需要
- 足够大的数据集和灵活的工具可能可以回答以前想象不到的问题

#### 经典数据处理系统

大数据挖掘系统稀少且昂贵的根本原因是，扩展一个系统以处理大数据集事非常困难；正如我们将看到的，它传统上受限于一台计算机的处理能力。但是随着数据大小的增加，有两种扩展系统的主要方法，通常称为向上扩展和向外扩展

- 向上扩展：在大多数企业中，数据处理通常是在价格高得惊人的大型计算机上执行的。随着数据大小的增长，方法是移动到更大的服务器或存储阵列。简单向上扩展的优势在于架构不会因为增长而发生重大变化。虽然用来较大的组件，但基本关系（例如，数据库服务器和存储阵列）保持不变。对于商业数据库引擎等应用，软件处理利用可用硬件的复杂性，但从理论上讲，通过将相同的软件迁移到越来越大的服务器上可以实现更大的规模。（但是实际上这个难度也不低）
- 向外扩展：单个主机大小也有实际限制，在某些情况下，向上扩展不能在进一步扩展。横向扩展方法将处理分散到越来越多的机器上，而不是将系统扩展到越来越大的硬件上。如果数据集翻了一番，只需要使用两台服务器，而不是一台双倍大小的服务器。这种方法的明显好处事，购买成本低得多。假设一台主机的价格是 5000 美元，但一台处理能力是其十倍的主机的价格可能是其一百倍。缺点是，我们需要制定策略将我们的数据处理分散到一组服务器上，而历史上证明这个策略也是复杂的

因此部署横向扩展解决方案需要大量的工程工作，系统开发人员需要手工制作用于数据分区和数据重组的机制，更不用说跨集群安排工作和处理单个机器故障的逻辑了

向上扩展的成本很高，向外扩展开发和管理系统的努力也是如此。限制因素主要有：

- 随着横向扩展系统变得越来越大，或者随着纵向扩展系统处理多个 CPU，系统中并发的复杂性带来的困难变得非常严重。有效利用多个主机或 CPU 是一项非常难的任务，实施必要的策略以在所需工作负载的整个执行过程中保持效率可能需要付出巨大的努力
- 系统能力的差异开始凸显。CPU 能力的增长速度远远快于网络或磁盘速度，CPU 周期曾经是系统中最有价值的资源，但是在今天这种情况已不再存在。与 20 年前相比，现代 CPU 能执行数百万倍的操作，但是内存和硬盘速度只增加了数千或数百倍。构建一个 CPU 强大的现代系统比较容易，以至于存储系统根本不能以足够快的速度向其提供数据，从而使 CPU 忙碌

从前面的场景来看，许多技术已经被成功用于减轻将数据处理系统扩展到大数据所需的痛苦，一些经验之谈如下：

##### 1. 不共享任何内容

共享的原则不适用于数据处理系统，而且这一思想既适用于数据，也适用于硬件。例如横向扩展体系结构中每个主机处理整个数据集的一个子集，以产生其最终结果的一部分。然后显示很少如此，主机之间可能需要相互通信，或者多个主机可能都需要某些数据。这些额外的依赖关系会给系统带来两方面的负面影响：瓶颈和故障。

- 瓶颈：如果系统中的每个计算都需要一条数据或单个服务器，则在相互竞争的客户端访问公共数据的主机时，存在争用和延迟的可能性。例如，如果在具有 25 台主机的系统中，只有一台主机必须由其它主机访问，则系统的整体性能将受该关键主机的功能限制
- 故障：如果保持关键数据的这个“热”服务器或存储系统出现故障，整个工作负载将崩溃。早期的集群解决方案经常显示出这种风险

系统的各个组件应该尽可能独立，而不是共享资源，无论其它组件是否被复杂的工作所束缚或者已经故障了，每个组件都可以继续工作

##### 2. 预计失败

我们可能会听到一种术语 “99.999% 的正常运行时间或可用性）”，虽然这绝对是同类中最好的可用性，但更重要的是认识到由此类设备组成的系统的总体可靠性可能由很大的差异，这取决于系统是否能够容忍单个组件故障

- 不能容忍单个组件故障：假设一台服务器具有 99% 的可靠性，系统需要 5 台这样的主机才能运行，则系统的可用性就是 0.99 的 5 次方，相当于 95% 的可用性。如果单个服务器的可靠性只有 95%，系统可靠性就会下降到只有 76%

- 能容忍单个组件故障：相反，如果构建的系统在任何给定时间只需要五台主机中的一台正常工作，则系统可用性将达到五个九的范围。 考虑与每个组件的关键程度相关的系统正常运行时间有助于将重点放在系统可用性可能达到的水平上。

99% 到底是什么程度的概念呢？ 例如，99%的可用性相当于每年的停机时间略高于 3.5 天或每月停机 7 小时。这样听起来好像 99% 也不是那么可靠了。预计失败，拥抱失败，解决失败。

##### 3. 智能软件，哑巴硬件

如果我们希望看到硬件集群以尽可能灵活的方式使用，为多个并行工作流提供托管，答案是将智能推向软件，而不是硬件。

在此模型中，硬件被视为一组资源，将硬件分配给特定工作负载的责任交给软件层。 这允许硬件是通用的，因此获得起来既容易又便宜，并且有效使用硬件的功能转移到软件上，而软件是关于有效执行该任务的知识所在。

##### 4. 移动处理，而不是数据

假设您有一个非常大的数据集，比如说 1000TB(即 1PB)，并且您需要对数据集中的每个数据执行一组四个操作。 让我们看看实现系统来解决这个问题的不同方式。

传统的大型纵向扩展解决方案将看到一台巨型服务器连接到同样令人印象深刻的存储系统，几乎可以肯定地使用光纤通道等技术来最大化存储带宽。 系统将执行该任务，但会受到 I/O 限制；即使是高端存储交换机也会限制将数据传送到主机的速度。

或者，以前集群技术的处理方法可能会看到一个由 1,000 台机器组成的集群，每台机器都有 1TB 的数据，分为四个象限，每个象限负责执行其中一个操作。 然后，集群管理软件将协调数据在集群中的移动，以确保每一块都接受所有四个处理步骤。 由于每条数据可以在其所在的主机上执行一个步骤，因此它将需要将数据流式传输到其他三个象限，因此我们实际上消耗了 3 PB 的网络带宽来执行处理。

请记住，处理能力的增长速度快于网络或磁盘技术，那么这些真的是解决问题的最佳方法吗？ 最近的经验表明答案是否定的，另一种方法是避免移动数据，而是移动处理。 使用刚才提到的集群，但不要将其划分为象限；相反，让 1000 个节点中的每个节点对本地保存的数据执行所有四个处理阶段。 如果幸运的话，您只需从磁盘流式传输数据一次，而通过网络传输的只有程序二进制文件和状态报告，这两者与实际数据集相比都相形见绌。

如果 1,000 个节点的群集听起来大得离谱，请考虑一下大数据解决方案所使用的一些现代服务器外形规格。 它们看到的是单个主机，每个主机中有多达 12 个 1 TB 或 2 TB 的磁盘。 因为现代处理器有多个核心，所以可以构建一个具有 1 PB 存储空间的 50 节点集群，同时仍然有一个 CPU 核心专门处理来自每个单独磁盘的数据流。

##### 5. 构建应用，而不是基础设施

在考虑上一节中的场景时，很多人都会关注数据移动和处理的问题。 但是，任何曾经构建过这样的系统的人都会知道，作业调度、错误处理和协调等不太明显的元素才是真正的魔力（难度）所在。

如果我们必须实现用于确定在哪里执行处理、执行处理并将所有子结果合并到整体结果中的机制，我们就不会从旧模型中获得太多好处。 在那里，我们需要显式地管理数据分区；我们只是在交换一个难题和另一个难题。

这涉及到最新的趋势，我们将在这里重点介绍：一个透明地处理大部分集群机制并允许开发人员从业务问题的角度进行思考的系统。 框架提供了定义良好的接口，这些接口抽象了所有这些复杂性-智能软件-在此基础上可以构建特定于业务领域的应用，从而提供了开发人员和系统效率的最佳组合。

#### Hadoop 的由来

前面都是在说一些集群的关键，但是还没回答 Hadoop 到底是什么

这一切都始于谷歌，它在 2003 年和 2004 年发布了两篇描述谷歌技术的学术论文：**Google 文件系统**(**gfs**)(http://research.google.com/archive/gfs.html)和 MapReduce(http://research.google.com/archive/mapreduce.html)。 这两者共同提供了一个以高效方式大规模处理数据的平台。

与此同时，Doug Cutting 正在开发 Nutch 开源网络搜索引擎。 他一直在研究系统中的元素，这些元素在 Google GFS 和 MapReduce 论文发表后引起了强烈共鸣。 Doug 开始了这些 Google 系统的实现工作，Hadoop 很快就诞生了，最初是 Lucene 的一个子项目，很快就成为了 Apache 开源基金会中自己的顶级项目。 因此，Hadoop 的核心是一个开源平台，它同时提供 MapReduce 和 GFS 技术的实现，并允许跨低成本商用硬件集群处理非常大的数据集。

雅虎在 2006 年聘请了 Doug Cutting，并很快成为 Hadoop 项目最著名的支持者之一。 除了经常宣传一些世界上最大的 Hadoop 部署外，雅虎还允许 Doug 和其他工程师在受雇期间为 Hadoop 做出贡献；雅虎还贡献了一些内部开发的 Hadoop 改进和扩展。 虽然道格现在已经转向 Cloudera(另一家支持 Hadoop 社区的知名初创公司)，雅虎 Hadoop 团队的大部分成员也被剥离出来，成立了一家名为 Hortonworks 的初创公司，但雅虎仍然是 Hadoop 的主要贡献者。

顶层 Hadoop 项目有许多组件子项目，我们将在本书中讨论其中几个，但主要的两个是**Hadoop 分布式文件系统**(**HDFS**)和 MapReduce。 这些都是 Google 自己的 GFS 和 MapReduce 的直接实现。 我们将对两者进行更详细的讨论，但目前，最好将 HDFS 和 MapReduce 视为一对互补但截然不同的技术。

**HDFS**是一个文件系统，它可以通过跨主机群集向外扩展来存储非常大的数据集。 它具有特定的设计和性能特征；尤其是，它针对吞吐量而不是延迟进行了优化，并且通过复制而不是冗余来实现高可用性。

**MapReduce**是一种数据处理范例，它指定数据将如何从其两个阶段(称为映射和还原)输入和输出，然后将其应用于任意大的数据集。 MapReduce 与 HDFS 紧密集成，确保 MapReduce 任务尽可能直接在保存所需数据的 HDFS 节点上运行。



### 2. 启动和运行 Hadoop

#### 安装 Hadoop

配置 Ubuntu 20, hadoop 3.3.1，模式选择的 伪分布式模式 ，参考教程: [Hadoop 3.3伪分布式环境搭建](https://www.jianshu.com/p/0ea28a25a7da), [hadoop3.3.1单机与伪分布安装](https://blog.csdn.net/weixin_45704680/article/details/120368821)，[启动和运行hadoop](https://bigdata2.apachecn.org/#/docs/hadoop-begin-guide/02)，最后个是个老版本的教程，但是好多命令行都是相似的，操作跟着教程走下来就行，踩到坑了就看日志，坑实在解决不了就换主机

记录下自己踩的坑

- 第一次下了一个hadoop 的源代码，里面并没有 bin 文件夹，然后 java 的版本没搞懂，jdk1.8 约等于 jdk8，有个 hadoop-env.sh 里面需要加入 JAVA_HOME 的路径
- 刚开始只有一个 jps 进程，需要先 stop-all.sh ，然后删除  tmp、logs 文件，将namenode文件夹格式化：hdfs namenode -format，最后重新启动
- 我重新启动之后查看进程，并没有 namenode，提示ip连接的问题，死活没解决，最后换了一个虚拟机和第一个教程 [Hadoop 3.3伪分布式环境搭建](https://www.jianshu.com/p/0ea28a25a7da)  成功了，管理界面不是 9000 端口，是 8088 了（第一台虚拟机的配置文件几乎一样，不知道为啥第一台机器还是不可以，来回安装了十几次吧，一晚上就这么没有了，猜测原因是第一个虚拟机的配置太差了 :cry:)







